{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13-May-25 18:28:24 - Running Mammoth! on head022. (if you see this message more than once, you are probably importing something wrong)\n",
      "[WARNING] 13-May-25 18:28:24 - Warning: python-dotenv not installed. Ignoring .env file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "usage: main.py --dataset\n",
      "               {rot-mnist,seq-chestx,seq-cifar100-224,seq-cub200,seq-cub200-rs,seq-cifar10-224-rs,seq-tinyimg,seq-tinyimg-r,seq-cars196,seq-cropdisease,seq-celeba,seq-cifar100-224-rs,seq-isic,seq-mnist,seq-cifar10-224,seq-cifar10,seq-mit67,seq-imagenet-r,seq-cifar100,mnist-360,perm-mnist,seq-resisc45,seq-eurosat-rgb}\n",
      "               --model\n",
      "               {er-tricks,lucir,er-ace,cnll,derpp,coda-prompt,ewc-on,lwf-mc,idefics,icarl-cscct,hal,zscl,er-ace-tricks,der,icarl,joint-gcl,slca,clip,icarl-casper,cgil,er-ace-lider,pnn,xder-ce,rpc,agem,attriclip,lwf,derpp-lider,second-order,moe-adapters,si,sgd,derpp-casper,derpp-cscct,dap,twf,gss,icarl-lider,xder-rpc-cscct,gdumb-lider,er,l2p,joint,gem,lws,er-ace-cscct,ranpac,xder-rpc,xder-rpc-casper,gdumb,llava,mer,fdr,starprompt,ccic,second-stage-starprompt,agem-r,puridiver,er-ace-aer-abs,xder,dualprompt,first-stage-starprompt,spr,bic,er-ace-casper}\n",
      "               [--backbone {resnet18,resnet18-7x7-pt,reduced-resnet18,resnet34,resnet32,vit,resnet50,resnet50-pt,mnistmlp}]\n",
      "               [--load_best_args]\n",
      "main.py: error: the following arguments are required: --dataset, --model\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "%run ./utils/main.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 13-May-25 18:28:58 - Running Mammoth! on head022. (if you see this message more than once, you are probably importing something wrong)\n",
      "[WARNING] 13-May-25 18:29:00 - Error in model clip\n",
      "[WARNING] 13-May-25 18:29:00 - Error in model icarl_casper\n",
      "[WARNING] 13-May-25 18:29:00 - Error in model cgil\n",
      "[WARNING] 13-May-25 18:29:01 - Error in model derpp_casper\n",
      "[WARNING] 13-May-25 18:29:01 - Error in model lws\n",
      "[WARNING] 13-May-25 18:29:02 - Error in model xder_rpc_casper\n",
      "[WARNING] 13-May-25 18:29:03 - Error in model starprompt\n",
      "[WARNING] 13-May-25 18:29:03 - Error in model second_stage_starprompt\n",
      "[WARNING] 13-May-25 18:29:03 - Error in model first_stage_starprompt\n",
      "[WARNING] 13-May-25 18:29:04 - Error in model er_ace_casper\n",
      "[WARNING] 13-May-25 18:29:04 - No default configuration found in models/config/der.yaml.\n",
      "[ERROR] 13-May-25 18:29:04 - Could not retrieve git hash.\n",
      "[INFO] 13-May-25 18:29:04 - \n",
      "╔═══════════════════════ CONFIGURATION ═══════════════════════╗\n",
      "║ Basic arguments:\n",
      "║   • backbone: resnet18\n",
      "║   • dataset: seq-cifar10\n",
      "║   • load_best_args: False\n",
      "║   • model: der\n",
      "║\n",
      "║ Configuration arguments:\n",
      "║   • model_config: default\n",
      "║\n",
      "║ Model-specific arguments:\n",
      "║   • alpha: 1.000000\n",
      "║\n",
      "║ Dataset arguments:\n",
      "║   • transform_type: weak\n",
      "║\n",
      "║ Backbone arguments:\n",
      "║   • num_filters: 64\n",
      "║\n",
      "║ Management arguments:\n",
      "║   • base_path: ./data/\n",
      "║   • code_optimization: 0\n",
      "║   • debug_mode: False\n",
      "║   • disable_log: False\n",
      "║   • distributed: no\n",
      "║   • enable_other_metrics: True\n",
      "║   • inference_only: False\n",
      "║   • non_verbose: False\n",
      "║   • permute_classes: False\n",
      "║   • results_path: results/\n",
      "║   • save_checkpoint_mode: safe\n",
      "║   • seed: 7\n",
      "║\n",
      "║ Wandb arguments:\n",
      "║   • wandb_entity: naima-elosegui-technische-universitat-berlin\n",
      "║   • wandb_name: shmp_test\n",
      "║   • wandb_project: mammoth\n",
      "║\n",
      "║ Experiment arguments:\n",
      "║   • batch_size: 32\n",
      "║   • eval_future: False\n",
      "║   • joint: 0\n",
      "║   • label_perc: 1.000000\n",
      "║   • label_perc_by_class: 1.000000\n",
      "║   • lr: 0.010000\n",
      "║\n",
      "║ Validation and fitting arguments:\n",
      "║   • early_stopping_epsilon: 0.000001\n",
      "║   • early_stopping_freq: 1\n",
      "║   • early_stopping_metric: loss\n",
      "║   • early_stopping_patience: 5\n",
      "║   • fitting_mode: epochs\n",
      "║   • n_epochs: 50\n",
      "║   • validation_mode: current\n",
      "║\n",
      "║ Optimizer and learning rate scheduler arguments:\n",
      "║   • drop_last: False\n",
      "║   • lr_milestones: []\n",
      "║   • optim_mom: 0.000000\n",
      "║   • optim_nesterov: False\n",
      "║   • optim_wd: 0.000000\n",
      "║   • optimizer: sgd\n",
      "║   • sched_multistep_lr_gamma: 0.100000\n",
      "║   • scheduler_mode: epoch\n",
      "║\n",
      "║ Noise arguments:\n",
      "║   • cache_path_noisy_labels: noisy_labels\n",
      "║   • disable_noisy_labels_cache: False\n",
      "║   • noise_rate: 0.000000\n",
      "║   • noise_type: symmetric\n",
      "║\n",
      "╚════════════════════════════════════════════════════════════╝\n",
      "[WARNING] 13-May-25 18:29:04 - Could not get memory from pynvml. Maybe try `pip install --force-reinstall gpustat`. module 'torch.cuda' has no attribute 'pynvml'\n",
      "[INFO] 13-May-25 18:29:04 - Using device cuda:0\n",
      "[INFO] 13-May-25 18:29:04 - Logging to wandb: naima-elosegui-technische-universitat-berlin/mammoth\n",
      "[WARNING] 13-May-25 18:29:04 - Label noise is not available with multi-label datasets. If this is not multi-label, ignore this warning.\n",
      "[INFO] 13-May-25 18:29:04 - Using backbone: resnet18\n",
      "[INFO] 13-May-25 18:29:04 - Using ResNet as backbone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaima-elosegui\u001b[0m (\u001b[33mnaima-elosegui-technische-universitat-berlin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nelosegui/BIFOLD_work/CL_curved/BlockOpt2GeomRep/mammoth_fork/wandb/run-20250513_182906-cvq4piys</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/mammoth/runs/cvq4piys' target=\"_blank\">shmp_test_5d770b27</a></strong> to <a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/mammoth' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/mammoth' target=\"_blank\">https://wandb.ai/naima-elosegui-technische-universitat-berlin/mammoth</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/naima-elosegui-technische-universitat-berlin/mammoth/runs/cvq4piys' target=\"_blank\">https://wandb.ai/naima-elosegui-technische-universitat-berlin/mammoth/runs/cvq4piys</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/CIFAR10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:07<00:00, 24345424.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/CIFAR10/cifar-10-python.tar.gz to ./data/CIFAR10\n",
      "[INFO] 13-May-25 18:29:19 - Using 4 workers for the dataloader.\n",
      "[INFO] 13-May-25 18:29:19 - Using 4 workers for the dataloader.\n",
      "[INFO] 13-May-25 18:29:19 - Evaluating model before task (for Forward Transfer metric)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Task 1: 100%|██████████| 63/63 [00:01<00:00, 40.90it/s, acc_task_1=49.9]\n",
      "Task 1 - Epoch 1:   1%|          | 84/15650 [00:20<1:12:47,  3.56it/s, loss=0.689, lr=0.01]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f791a705f50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Task 1 - Epoch 1:   1%|          | 85/15650 [00:20<1:15:04,  3.46it/s, loss=0.687, lr=0.01]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 2449070, 2449072, 2449074) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(res)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/multiprocessing/reductions.py:496\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_storage_fd\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, size):\n\u001b[0;32m--> 496\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/resource_sharer.py:57\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_resource_sharer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduction\u001b[38;5;241m.\u001b[39mrecv_handle(conn)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/resource_sharer.py:86\u001b[0m, in \u001b[0;36m_ResourceSharer.get_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     85\u001b[0m address, key \u001b[38;5;241m=\u001b[39m ident\n\u001b[0;32m---> 86\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m c\u001b[38;5;241m.\u001b[39msend((key, os\u001b[38;5;241m.\u001b[39mgetpid()))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/connection.py:519\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 519\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[43mSocketClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m authkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(authkey, \u001b[38;5;28mbytes\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/connection.py:647\u001b[0m, in \u001b[0;36mSocketClient\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    646\u001b[0m s\u001b[38;5;241m.\u001b[39msetblocking(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 647\u001b[0m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Connection(s\u001b[38;5;241m.\u001b[39mdetach())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/BIFOLD_work/CL_curved/BlockOpt2GeomRep/mammoth_fork/main.py:436\u001b[0m\n\u001b[1;32m    432\u001b[0m     train(model, dataset, args)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 436\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BIFOLD_work/CL_curved/BlockOpt2GeomRep/mammoth_fork/main.py:432\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 432\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BIFOLD_work/CL_curved/BlockOpt2GeomRep/mammoth_fork/utils/training.py:248\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataset, args)\u001b[0m\n\u001b[1;32m    244\u001b[0m model\u001b[38;5;241m.\u001b[39mmeta_begin_epoch(epoch, dataset)\n\u001b[1;32m    246\u001b[0m train_pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 248\u001b[0m \u001b[43mtrain_single_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_pbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msystem_tracker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_tracker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m model\u001b[38;5;241m.\u001b[39mmeta_end_epoch(epoch, dataset)\n\u001b[1;32m    253\u001b[0m epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/BIFOLD_work/CL_curved/BlockOpt2GeomRep/mammoth_fork/utils/training.py:86\u001b[0m, in \u001b[0;36mtrain_single_epoch\u001b[0;34m(model, train_loader, args, epoch, pbar, system_tracker, scheduler)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_iter)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1144\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1143\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 2449070, 2449072, 2449074) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "%run ./main.py --model der --dataset seq-cifar10 --backbone resnet18 --buffer_size 200 --alpha 1 --lr 1e-2 --wandb_name shmp_test --wandb_entity naima-elosegui-technische-universitat-berlin --wandb_project mammoth --enable_other_metrics 1 --seed 7  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
